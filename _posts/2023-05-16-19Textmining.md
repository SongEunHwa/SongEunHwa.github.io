---
layout: single
title:  "19.LDA 분석과 결과의 해석"
categories: TextMining
tag: [python,Jupyter Notebook,TextMining]
toc: true
toc_sticky: true
author_profile: false #옆에 정보 끄기
sidebar: 
    nav: "docs"
typora-root-url: ../
---

#### 19강 LDA 분석과 결과의 해석

##### 01. 패키지와 데이터 불러오기

```python
import pandas as pd

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

import gensim
from gensim import corpora
# from sklearn.externals import joblib
import joblib
import sklearn
sklearn.externals.joblib = joblib
```

16,17강에서 불러왔던 동일한 패키지를 불러오고,

여기서 gensim 패키지에서 corpora를 불러오는데 이는 분석 시에 이전과 같은 word형태로 묶은 것이 아닌 corpus 단위로 저장하기 위해서 사용한다.

```python
en_data=pd.read_csv('wos_ai_.csv', encoding='euc-kr')
en_data_abstract = en_data['ABSTRACT']
```

실습파일을 불러온다. 여기서 'ABSTRACT'열에 해당되는 부분만 en_data_abstract 변수에 저장해준다.



###### 02. 전처리 및 corpus 준비

```python
en_doc = []    #문서집
en_word_joined = []   #문장집
en_word = []    #단어집
```

전처리를 위한 문서집, 문장집, 단어집에 해당되는 변수를 각각 만들어준다

```python
for doc in en_data_abstract:
    if type(doc) != float:   #숫자가 아닌 문서에 해당되는 부분만
        en_doc.append(doc.replace("-", " ")) #"-"을 공백으로 바꿔 저장

en_stopwords = set(stopwords.words("english")) #불용어 사전 저장
en_stemmer = PorterStemmer()  #어간추출을 위한 명령어 저장

#전처리 과정
for doc in en_doc: 
    en_alphabet = re.sub(r"[^a-zA-Z]+", " ", str(doc)) #알파벳만 추출
    en_tokenized = word_tokenize(en_alphabet.lower()) #소문자화, 토큰화
    en_stopped = [w for w in en_tokenized if w not in en_stopwords] #불용어처리
    en_stemmed = [en_stemmer.stem(w) for w in en_stopped] #어간추출
    en_word_joined.append(' '.join(en_stemmed)) #join해서 문장별로 저장
    en_word.append(en_stemmed) #join하지 않고 전처리된 단어를 저장
```

